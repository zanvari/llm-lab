# ğŸ§ª Prompt Engineering Playground

This project explores different prompt styles and their effect on LLM outputs. It provides a simple interface to experiment with:

- ğŸ”¹ Zero-shot prompts
- ğŸ”¸ Few-shot prompts
- ğŸ” Chain-of-thought reasoning

## ğŸ“˜ Notebook
- [`prompt_playground.ipynb`](./prompt_playground.ipynb): Compare outputs from multiple LLMs on various prompt strategies.

## ğŸ“‚ Structure
```
prompt-playground/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ prompts.json             # Example prompts (task, type, input)
â”œâ”€â”€ outputs/                     # Saved logs or comparisons
â”œâ”€â”€ prompt_playground.ipynb      # Main notebook
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## ğŸ§  Models Used (Simulated)
- OpenAI (e.g., GPT-4)
- Mistral
- LLaMA

You can integrate real APIs by replacing the simulated `query_*` functions in the notebook.

## ğŸ“¦ Requirements
See `requirements.txt` for dependencies.
