# ğŸ”— BERT Sentence Similarity

This project fine-tunes BERT to score similarity between pairs of sentences using the STS-B dataset (GLUE benchmark).

## ğŸ“˜ Notebook
- [`similarity_finetune.ipynb`](./notebooks/similarity_finetune.ipynb) â€“ Fine-tuning, evaluation, and inference
- [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zanvari/llm-lab/blob/main/bert-sentence-similarity/notebooks/similarity_finetune.ipynb)

## ğŸ“š Dataset
- **GLUE STS-B**: Semantic Textual Similarity Benchmark
- Sentence pairs with similarity scores from 0 to 5 (normalized to [0, 1])

## ğŸ§  Model
- `bert-base-uncased` fine-tuned using HuggingFace Trainer
- Regression head (single output neuron)

## ğŸ§ª Evaluation
- Metrics: Pearson and Spearman correlation
- Inference via cosine similarity or regression output

## ğŸ“ Structure
```
bert-sentence-similarity/
â”œâ”€â”€ data/                  # Input datasets
â”œâ”€â”€ notebooks/             # Jupyter notebook
â”œâ”€â”€ outputs/               # Model checkpoints
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

## ğŸš€ Usage
```bash
pip install -r requirements.txt
```

## ğŸ“„ License
MIT


---

## ğŸ› ï¸ Inference & Evaluation

### ğŸ” Single Pair
```bash
python predict.py
```

### ğŸ“¦ Batch Inference
```bash
python predict_batch.py
```
- Input: `data/batch_input.json`
- Output: `outputs/batch_output.json`

### ğŸ“Š Evaluation
```bash
python evaluate.py
```
- Compares predictions with labels
- Reports Pearson and Spearman correlation

---

## ğŸ“¦ Example Input
```json
[
  {
    "id": "1",
    "sentence1": "A man is playing a guitar.",
    "sentence2": "Someone is performing music.",
    "label": 0.9
  }
]
```
