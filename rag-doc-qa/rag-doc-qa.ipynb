{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d946fc53",
   "metadata": {},
   "source": [
    "\n",
    "# üß† Retrieval-Augmented Generation (RAG) for Document QA\n",
    "\n",
    "This notebook demonstrates how to build a complete **RAG-based document question answering pipeline** using LangChain. It covers:\n",
    "- Loading and chunking single/multiple PDFs\n",
    "- Embedding with OpenAI and HuggingFace\n",
    "- Vector storage with FAISS and Chroma\n",
    "- Retrieval with similarity and MMR\n",
    "- Generation with OpenAI GPT and HuggingFace (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd436f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install langchain openai faiss-cpu tiktoken PyPDF2 chromadb sentence-transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67463ed3",
   "metadata": {},
   "source": [
    "## üì¶ Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feafe4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS, Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import pipeline\n",
    "import os\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab7627",
   "metadata": {},
   "source": [
    "## üîê Load API Keys from .env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c46932",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9764a698",
   "metadata": {},
   "source": [
    "## üìÑ Load a Single PDF File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e4464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = PyPDFLoader(\"data/contract_detailed.pdf\")\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} pages from contract_detailed.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755dab2f",
   "metadata": {},
   "source": [
    "## üìÅ Load Multiple PDF Files from Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6de3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "all_docs = []\n",
    "data_path = Path(\"data\")\n",
    "for file in data_path.glob(\"*.pdf\"):\n",
    "    docs = PyPDFLoader(str(file)).load()\n",
    "    all_docs.extend(docs)\n",
    "print(f\"Loaded total {len(all_docs)} pages from {len(list(data_path.glob('*.pdf')))} PDFs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc37261",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Split Documents into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e7070",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(all_docs)\n",
    "print(f\"Total chunks created: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf26958",
   "metadata": {},
   "source": [
    "## üî° Create Embeddings\n",
    "We compare two methods: OpenAI and HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c00ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_oa = OpenAIEmbeddings()\n",
    "embedding_hf = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78627be",
   "metadata": {},
   "source": [
    "## üíæ Store Vectors in FAISS and Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a54c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "faiss_store = FAISS.from_documents(chunks, embedding_oa)\n",
    "chroma_store = Chroma.from_documents(chunks, embedding_hf, collection_name=\"rag-demo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e126fb73",
   "metadata": {},
   "source": [
    "## üîç Create Retrievers (Similarity and MMR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57848836",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "faiss_retriever = faiss_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "mmr_retriever = chroma_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a2da5e",
   "metadata": {},
   "source": [
    "## üß† Setup LLMs (OpenAI GPT-3.5 Turbo, Optional HF Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf196b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_oa = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "# Optional: HuggingFace (if using local models)\n",
    "# hf_pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "# llm_hf = HuggingFacePipeline(pipeline=hf_pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68f024",
   "metadata": {},
   "source": [
    "## üîó Build RetrievalQA Chains with Different Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3d4ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa_openai_faiss = RetrievalQA.from_chain_type(llm=llm_oa, retriever=faiss_retriever, return_source_documents=True)\n",
    "qa_openai_mmr = RetrievalQA.from_chain_type(llm=llm_oa, retriever=mmr_retriever, return_source_documents=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e187e37",
   "metadata": {},
   "source": [
    "## ‚ùì Ask a Query and Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500dfb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"What are the termination conditions in the contract?\"\n",
    "result_faiss = qa_openai_faiss({\"query\": query})\n",
    "result_mmr = qa_openai_mmr({\"query\": query})\n",
    "\n",
    "print(\"FAISS + OpenAI Answer:\\n\", result_faiss['result'])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result_faiss['source_documents']:\n",
    "    print(\"-\", doc.metadata.get(\"source\", \"N/A\"))\n",
    "\n",
    "print(\"\\nMMR + OpenAI Answer:\\n\", result_mmr['result'])\n",
    "print(\"\\nSources:\")\n",
    "for doc in result_mmr['source_documents']:\n",
    "    print(\"-\", doc.metadata.get(\"source\", \"N/A\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b949cf",
   "metadata": {},
   "source": [
    "\n",
    "## üìä Summary: Pros & Cons\n",
    "\n",
    "| Component         | Option A         | Option B          | Notes |\n",
    "|------------------|------------------|-------------------|-------|\n",
    "| Embeddings       | OpenAI           | HuggingFace       | OpenAI is more accurate; HF is free/local |\n",
    "| Vector Store     | FAISS            | Chroma            | FAISS is fast/local; Chroma supports MMR |\n",
    "| Retriever Type   | Similarity       | MMR               | MMR adds diversity in context |\n",
    "| Generator (LLM)  | OpenAI GPT-3.5   | HF Instruct Model | OpenAI is reliable; HF can be self-hosted |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445dbc0d",
   "metadata": {},
   "source": [
    "## üíæ Save Vector Store for Reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss_store.save_local('vectorstore/faiss')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
