{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec251a9f",
   "metadata": {},
   "source": [
    "# üìö Summarization Benchmarking\n",
    "\n",
    "Compare extractive vs abstractive summarization methods using traditional NLP and LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1316521",
   "metadata": {},
   "source": [
    "## üìÑ Load Sample Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e7fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/sample_article.txt\") as f:\n",
    "    article = f.read()\n",
    "\n",
    "print(article)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b36ca6",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Extractive Summarization (TextRank via spaCy + pytextrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7301660b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pytextrank\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"textrank\")\n",
    "\n",
    "doc = nlp(article)\n",
    "extractive_summary = doc._.textrank.summary(limit_phrases=10, limit_sentences=3)\n",
    "for sent in extractive_summary:\n",
    "    print(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c617d64e",
   "metadata": {},
   "source": [
    "## üß† Abstractive Summarization (FLAN-T5 via Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dae7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "abstractive = pipeline(\"summarization\", model=\"google/flan-t5-base\", tokenizer=\"google/flan-t5-base\")\n",
    "ab_summary = abstractive(article, max_length=100, min_length=30, do_sample=False)[0]['summary_text']\n",
    "print(ab_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878983d",
   "metadata": {},
   "source": [
    "## üìè ROUGE Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da790e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(ab_summary, ' '.join([str(s) for s in extractive_summary]))\n",
    "print(scores)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
