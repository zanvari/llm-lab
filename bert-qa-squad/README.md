# â“ BERT QA Fine-tuning â€” SQuAD

Fine-tune `bert-base-uncased` on the Stanford Question Answering Dataset (SQuAD v1.1) using HuggingFace Transformers.

---

## ğŸ“ Structure
- `notebooks/qa_finetune.ipynb` â€” Full notebook: training, evaluation, prediction
- `data/` â€” (Optional) store preprocessed or downloaded data
- `outputs/` â€” Saved fine-tuned model checkpoints

---

## âœ… Goals
- Load and tokenize SQuAD v1.1
- Train BERT for extractive QA
- Evaluate EM/F1 on the validation set
- Run inference on new questions + context

---

## ğŸ§ª Model
- `bert-base-uncased`
- Architecture: `BertForQuestionAnswering`

---

## ğŸ”§ Setup
```bash
pip install -r requirements.txt
```

---

## ğŸ“¦ Dependencies
- transformers
- datasets
- torch
- tqdm
- scikit-learn
