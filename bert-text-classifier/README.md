# ğŸ§  BERT Text Classifier

This project fine-tunes BERT and other transformer-based models for text classification using HuggingFace Transformers and the IMDb dataset.

---

## ğŸ“ Structure
- `notebooks/bert_classifier.ipynb` â€“ Fine-tune BERT on binary classification
- `notebooks/bert_comparison.ipynb` â€“ Compare BERT, DistilBERT, and RoBERTa with confusion matrix
- `train.py` â€“ Training script (coming soon)
- `infer.py` â€“ Inference script (coming soon)
- `data/` â€“ Store raw or preprocessed datasets here

---

## âœ… Features
- Load and tokenize text datasets (IMDb)
- Fine-tune `bert-base-uncased`, `distilbert-base-uncased`, and `roberta-base`
- Plot confusion matrix and classification report
- Evaluate using accuracy and F1-score

---

## ğŸ“Š Model Comparison
| Model               | Type       | Notes                      |
|--------------------|------------|----------------------------|
| `bert-base-uncased`| Base BERT  | Standard benchmark         |
| `distilbert-base`  | Lightweight| Faster, smaller version    |
| `roberta-base`     | Robust     | Better performance on many NLP tasks |

---

## ğŸš€ Usage
Run the comparison notebook to train and evaluate:
```bash
notebooks/bert_comparison.ipynb
```

---

## ğŸ”§ Requirements
```bash
transformers
datasets
scikit-learn
torch
seaborn
matplotlib
```
